
<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Kai Li</title>
	<meta content="Kai Li, kailigo.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
a {
  color: #1772d0;
  text-decoration:none;
}
a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}
a.paper {
  font-weight: bold;
  font-size: 14pt;
}
b.paper {
  font-weight: bold;
  font-size: 14pt;
}
* {
  margin: 0pt;
  padding: 0pt;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 900px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  background: #eee;
}
h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 17pt;
  font-weight: 700;
}
h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 18px;
  font-weight: 700;
}
strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15px;
  font-weight:bold;
}
ul { 
  list-style: circle;
}
img {
  border: none;
}
li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}
alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15px;
  font-weight: bold;
  color: #FF0000;
}
em, i {
	font-style:italic;
}
div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}
div.spanner {
  clear: both;
}
div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}
div.paper div {
  padding-left: 230px;
}
img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}
span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}
pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}
div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-45959174-3', 'kailigo.github.io');
  ga('send', 'pageview');
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');
</script>
<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
<img title="KaiLi" style="float: left; padding-left: .01em; height: 140px;" src="KaiLi.jpg" />
<div style="padding-left: 12em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Kai Li</span><br />
<span><strong>Researcher</strong></span><br />
<!--<span><a href='http://cvrs.whu.edu.cn/'>Computer Vision & Remote Sensing (CVRS) Lab </a> <br /> </q>-->
<span>Meta Reality Labs.</span><br />
<span><strong>Office</strong>: </span><br />
<span><strong>Email  </strong>: li.gm.kai [at] gmail.com  &nbsp &nbsp  
<strong><a href='https://github.com/kailigo'>Github</a></strong>  &nbsp &nbsp
<strong><a href='https://scholar.google.com/citations?hl=en&user=YsROc4UAAAAJ'>Google Scholar</a></strong></span><br/>
<!-- <span><strong> <a href='https://github.com/kailigo'>Github</a> </strong></span> <br /> -->
<!-- <span><strong> <a href='https://scholar.google.com/citations?hl=en&user=YsROc4UAAAAJ'>Google Scholar</a> </strong></span> <br /> -->
</div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
<div class="section">
<h2>About Me</h2>
<div class="paper">
I am a research scientist in Meta. Prior to this, I was a researcher in 
<a href='https://www.nec-labs.com/research-departments/machine-learning/machine-learning-home'>the Machine Learning Department of NEC Labs, America</a>.
I obtained my PhD from <a href='http://www.northeastern.edu/'>Northeastern University</a>, and my master and bachelor degrees from <a href='http://www.whu.edu.cn/'>Wuhan University</a>. I interned in <a href='https://research.adobe.com/'>Adobe Research</a>, <a href='https://www.nec-labs.com/'>NEC Labs</a>, and <a href='https://air.jd.com/'>JD AI Research, America.</a>
My research interests lie in machine learning and computer vision, particularly in label-efficient learning (e.g., zero/few-shot learning, domain adaptation), video understanding (e.g., action recognition, action detection), visual content generation (text-to-image/video generation, image-to-video generation). <p> 
	
<!-- <br><strong> The Machine Learning Department of NEC Labs, America is hiring <u>full-time</u> researchers. Contact me if you are interested</strong> -->

</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
  <li> 2024.01: Join Meta as a research scientist.</li>	    
  <li> 2023.09: One paper accepted by NeurIPS 2023.</li>	
  <li> 2023.07: Three papers accepted by ICCV 2023.</li>	    
  <li> 2023.04: One paper accepted by MICCAI 2023 and one paper by ICASSP 2023.</li>
  <li> 2023.02: Three papers accepted by CVPR 2023.</li>	   
  <li> 2023.01: One paper accepted by AAAI 2023.</li>
  <li> 2022.12: Two medical journal papers accepted by Bioinformatics and Frontiers in Immunology.</li>
  <li> 2022.02: One paper accepted by CVPR 2022.</li> 	    
  <li> 2022.02: One paper accepted by IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI).</li> 	    
  <li> 2021.07: Two papers accepted by ICCV 2021.</li> 
  <li> 2021.07: Two papers accepted by ACM MM 2021.</li>          
  <li> 2021.06: One paper accepted by IEEE Trans. on Image Processing (TIP).</li>          
  <li> 2021.06: Join NEC Labs, America as a researcher.</li>          
  <li> 2021.06: One paper accepted by IEEE Trans. on Cybernetics.</li>          
  <li> 2021.04: Defense my PhD.</li>          
  <li> 2021.03: One paper accepted by CVPR 2021.</li>        
  <li> 2020.12: One paper accepted by CHI 2021.</li>        
  <li> 2020.11: Receive the Dissertation Completion Fellowship from the university.</li>    
  <li> 2020.09: One paper accepted by TNNLS.</li>    
  <li> 2020.06: One paper accepted by IEEE Trans. on Big Data (TBD).</li>    
  <li> 2020.05: Receive the Outstanding Research Assistant Award from College of Engineering.</li>    
  <li> 2020.02: Two papers accepted by CVPR 2020.</li>    
  <li> 2019.08: One paper accepted by CIKM 2019.</li>
  <li> 2019.07: Three papers are accepted by ICCV 2019.</li>     
  <li> 2018.12: One paper accepted by ICLR 2019.</li>       
  <li> 2018.09: One paper accepted by TNNLS.</li>   
  <li> 2018.07: One paper accepted by ACM MM 2018 as full long paper.</li>   
  <li> 2018.07: One paper accepted by ECCV 2018.</li>    
  <li> 2017.11: Two papers accepted by AAAI18.</li>   
	<li> 2016.09: Begin my new study journey in Northeastern University at Boston.</li>   
    </ul>
  </div>
</div>
</div>




<div style="clear: both;">
<div class="section">
  <h2>Recent Publications [<a href='./pubs.html'>Full list</a>] [<a href='https://scholar.google.com/citations?hl=en&user=YsROc4UAAAAJ'>Google Scholar</a>]
  </h2>
  <div class="paper">

<ul>
<li><p> Conditional Image-to-Video Generation with Latent Flow Diffusion Models. [PDF]
<br>
Haomiao Ni, Changhao Shi, <u>Kai Li</u>, Sharon X. Huang, and Martin Renqiang Min.
<br>
<i> IEEE International Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. </i></p>
</li>
</ul>	  
	  
<ul>
<li><p> Source-Free Video Domain Adaptation with Spatial-Temporal-Historical Consistency Learning [PDF]
<br>
<u>Kai Li</u>, Deep Patel, Erik Kruus, and Martin Renqiang Min.
<br>
<i> IEEE International Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. </i></p>
</li>
</ul>

<ul>
<li><p> Camouflaged Object Detection with Feature Decomposition and Edge Reconstruction [PDF]
<br>
Chunming He, <u>Kai Li</u>, Yachao Zhang, Longxiang Tang, Yulun Zhang, Zhenhua Guo, and Xiu Li.
<br>
<i> IEEE International Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. </i></p>
</li>
</ul>
	  	
	  
<ul>
<li><p> StyleT2I: Toward Compositional and High-Fidelity Text-to-Image Synthesis. [PDF]
<br>
Zhiheng Li, Martin Renqiang Min, <u>Kai Li</u>, and Chenliang Xu.
<br>
<i> IEEE International Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022. </i></p>
</li>
</ul>
	  
	  
<ul>
<li><p> Semi-Supervised Domain Adaptation with Prototypical Alignment and Consistency Learning <a href='https://openaccess.thecvf.com/content/ICCV2021/papers/Li_ECACL_A_Holistic_Framework_for_Semi-Supervised_Domain_Adaptation_ICCV_2021_paper.pdf'>[PDF]</a><a href='https://github.com/kailigo/pacl'>[code]</a>
<br>
<u>Kai Li</u>, Chang Liu, Handong Zhao, Yulun Zhang, Yun Fu.
<br>
<i> International Conference on Computer Vision (<strong>ICCV</strong>), 2021. </i></p>
</li>
</ul>
	  
<ul>
<li><p> MR Image Super-Resolution With Squeeze and Excitation Reasoning Attention Network <a href='https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_MR_Image_Super-Resolution_With_Squeeze_and_Excitation_Reasoning_Attention_Network_CVPR_2021_paper.pdf'>[PDF]</a>
<br>
Yulun Zhang, <u>Kai Li</u>, Kunpeng Li, Yun Fu.
<br>
<i>IEEE International Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2021. </i></p>
</li>
</ul>
	  
	  
<ul>
<li><p>Cross-Domain Document Object Detection: Benchmark Suite and Method. <a href='https://arxiv.org/pdf/2003.13197.pdf'>[PDF]</a><a href='https://github.com/kailigo/cddod'>[code]</a> <br>
<u>Kai Li</u>, C. Wigington, C. Tensmeyer, H. Zhao, N. Barmpalios, V. Morariu, V. Manjunatha, T. Sun, Yun Fu. <br> 
<i>IEEE International Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020. </i></p>
</li>
</ul>

	  
<ul>
<li><p>Adversarial Feature Hallucination Networks for Few-Shot Learning. <a href='https://arxiv.org/pdf/2003.13193.pdf'>[PDF]</a>[code] <br>
<u>Kai Li</u>, Yulun Zhang, Kunpeng Li, Yun Fu. <br> 
<i>IEEE International Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020. </i></p>
</li>
</ul>
	  
<ul>
<li><p> Attention Bridging Network for Knowledge Transfer. <a href='http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Attention_Bridging_Network_for_Knowledge_Transfer_ICCV_2019_paper.pdf'>[PDF]</a>[code]<br>
<!-- <u>Kai Li</u>, Martin Renqiang Min, Yun Fu. -->
Kunpeng Li, Yulun Zhang, <u>Kai Li</u>, Yuanyuan Li, Yun Fu.
<br>
<i> International Conference on Computer Vision (<strong>ICCV</strong>), 2019. </i></p>
</li>
</ul>

	  	  
<ul>
<li><p> Visual Semantic Reasoning for Image-Text Matching. <a href='https://arxiv.org/pdf/1909.02701.pdf'>[PDF]</a><a href='https://github.com/KunpengLi1994/VSRN'>[code]</a>
<br>
Kunpeng Li, Yulun Zhang, <u>Kai Li</u>, Yuanyuan Li, Yun Fu.
<br>
<i> International Conference on Computer Vision (<strong>ICCV</strong>), 2019. </i></p>
</li>
</ul>	  
	  




<ul>
<li><p> Rethinking Zero-Shot Learning: A Conditional Visual Classification Perspective. <a href='https://arxiv.org/pdf/1909.05995.pdf'>[PDF]</a><a href='https://github.com/kailigo/cvcZSL'>[code]</a><br>
<u>Kai Li</u>, Martin Renqiang Min, Yun Fu.<br>
<i> International Conference on Computer Vision (<strong>ICCV</strong>), 2019. </i></p>
</li>
</ul>



  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Selected Awards</h2>
<div class="paper">
    <ul>
  <li> Dissertation Completion Award, Northeastern University, 2020. </li>
  <li> Outstanding Research Assistant Award, College of Engineering, Northeastern University, 2020. </li>
	<li> Dean's Fellowship, College of Engineering, Northeastern University, 2016. </li>
	<li> Wang Zhizhou Scholarship, Wuhan University, 2015. </li>
    <li> Excellent Graduate Student, Wuhan University, 2015. </li>
    <li> Outstanding Undergraduate Thesis, Hubei Province, 2015. </li>	
    <li> Best Undergraduate Thesis, School of Remote Sensing and Information Engineering, Wuhan University, 2014. </li>
    <!-- <li> Excellent Undergraduate Students, Wuhan University, 2012. </li> -->
    </ul>
</ul>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Professonal Activities</h2>
<div class="paper">
<ul>
<p><font size="5">
	<li> Reviewer for TPAMI, TIP, TNNLS, TKDD, IET Image Processing, JEI, Neurocomputing</li>    	 	
	<li> Reviewer for CVPR, ECCV, ICCV, ICLR, NeurIPS, IJCAI, AAAI, WACV. </li>
    <!-- <li> Guest reviewer for AAAI 2017. </li>    	  -->
	<li> IEEE student member, AAAI student member. </li>   
</font></p>
</ul>
</div>
</div>
</div>



<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 11th Dec, 2017</a></font></p>
<p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div>

<!-- <hr> -->
<!-- <!--  -->
<div id="clustrmaps-widget"></div><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=VWJ0&d=U3QgYTohL0BDKnd1NmsAJrR-Sr6wSsc3wxseuw4wyD4"></script>
<!-- -->
</body>
</html>
